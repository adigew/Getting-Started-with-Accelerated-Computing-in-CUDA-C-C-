{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a report file which can be used in a variety of manners, including for use in visual profiling with Nsight Systems, which we will look at in more detail in the following section.\n",
    "\n",
    "Here we use the `--stats=true` flag to indicate we would like summary statistics printed. In this section this summary will be the focus of our attention. There is quite a lot of information printed:\n",
    "\n",
    "- Operating System Runtime Summary (`osrt_sum`)\n",
    "- **CUDA API Summary (`cuda_api_sum`)**\n",
    "- **CUDA Kernel Summary (`cuda_gpu_kern_sum`)**\n",
    "- **CUDA Memory Time Operation Summary (`cuda_gpu_mem_time_sum`)**\n",
    "- **CUDA Memory Size Operation Summary (`cuda_gpu_mem_size_sum`)**\n",
    "\n",
    "In this section you will primarily be using the 4 summaries in **bold** above. In the next section, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `cuda_gpu_kern_sum` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-14fa.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.3       6053933434        317  19097581.8  10071878.0      1840  100154357   27332108.1  poll                  \n",
      "      8.9        594680679        283   2101345.2   2064414.0       120   20429370    1424518.7  sem_timedwait         \n",
      "      0.5         31671327        499     63469.6     11150.0       380    7939262     377485.5  ioctl                 \n",
      "      0.3         19865890         24    827745.4      4615.0       850    7192390    2227989.6  mmap                  \n",
      "      0.0           881923         27     32663.8      4410.0      2990     537309     101683.8  mmap64                \n",
      "      0.0           478318         44     10870.9     10825.0      3600      30821       4430.2  open64                \n",
      "      0.0           165352          4     41338.0     38525.5     31220      57081      12035.9  pthread_create        \n",
      "      0.0           142572         11     12961.1     13700.0       840      18030       4639.4  write                 \n",
      "      0.0           134802         29      4648.3      3221.0      1480      20720       4117.7  fopen                 \n",
      "      0.0            60743         11      5522.1      3510.0      1701      18081       5120.5  munmap                \n",
      "      0.0            50851         26      1955.8        70.0        60      49081       9611.7  fgets                 \n",
      "      0.0            35851         52       689.4       520.0       160       6510        872.7  fcntl                 \n",
      "      0.0            33091          6      5515.2      5515.0      2500       7960       2171.1  open                  \n",
      "      0.0            25330         22      1151.4       970.0       550       3770        672.1  fclose                \n",
      "      0.0            21741         14      1552.9      1205.5       780       4050        998.4  read                  \n",
      "      0.0            16710          2      8355.0      8355.0      5490      11220       4051.7  socket                \n",
      "      0.0            12200          1     12200.0     12200.0     12200      12200          0.0  connect               \n",
      "      0.0             8081          5      1616.2      1351.0        70       3300       1473.5  fread                 \n",
      "      0.0             6230          1      6230.0      6230.0      6230       6230          0.0  pipe2                 \n",
      "      0.0             5290         64        82.7        50.0        40        190         46.6  pthread_mutex_trylock \n",
      "      0.0             2540          1      2540.0      2540.0      2540       2540          0.0  bind                  \n",
      "      0.0             1210          1      1210.0      1210.0      1210       1210          0.0  listen                \n",
      "      0.0              280          1       280.0       280.0       280        280          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.1       2471506658          1  2471506658.0  2471506658.0  2471506658  2471506658          0.0  cudaDeviceSynchronize\n",
      "      4.2        108272853          3    36090951.0       30510.0       16091   108226252   62471003.6  cudaMallocManaged    \n",
      "      0.8         19932312          3     6644104.0     6577719.0     6118312     7236281     561933.2  cudaFree             \n",
      "      0.0            46621          1       46621.0       46621.0       46621       46621          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2471496420          1  2471496420.0  2471496420.0  2471496420  2471496420          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34138263   2304   14817.0    4351.5      1982     80224      22490.4  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11062385    768   14404.1    3759.5      1279     80544      22784.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-c4e6.qdstrm'\n",
      "[1/8] [========================100%] report1.nsys-rep\n",
      "[2/8] [========================100%] report1.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6154343273        318  19353280.7  10074122.0      2480  100175395   27648662.0  poll                  \n",
      "      8.7        594302075        283   2100007.3   2065185.0       120   20480740    1370120.5  sem_timedwait         \n",
      "      0.5         35976679        499     72097.6     12410.0       510    9199323     436415.2  ioctl                 \n",
      "      0.3         19171110         24    798796.3      8840.0       820    7195710    2148526.1  mmap                  \n",
      "      0.0          1101110         27     40781.9      4060.0      3210     705562     133858.6  mmap64                \n",
      "      0.0           496951         44     11294.3     10960.0      4460      33071       4281.1  open64                \n",
      "      0.0           181581         29      6261.4      3570.0      1500      46850       8528.5  fopen                 \n",
      "      0.0           138540          4     34635.0     34565.0     24430      44980      10685.7  pthread_create        \n",
      "      0.0           130232         11     11839.3     11490.0       990      17751       5433.0  write                 \n",
      "      0.0            99280         11      9025.5      4500.0      1590      35250      11560.4  munmap                \n",
      "      0.0            66782          6     11130.3      8325.0      3281      30851       9900.2  open                  \n",
      "      0.0            57311         26      2204.3        80.0        70      55171      10803.1  fgets                 \n",
      "      0.0            35690         52       686.3       510.0       210       5570        759.6  fcntl                 \n",
      "      0.0            29470         22      1339.5      1195.0       710       3170        602.6  fclose                \n",
      "      0.0            23011         14      1643.6      1520.0       440       4060       1088.7  read                  \n",
      "      0.0            16140          2      8070.0      8070.0      4200      11940       5473.0  socket                \n",
      "      0.0            12771          1     12771.0     12771.0     12771      12771          0.0  connect               \n",
      "      0.0            12431          5      2486.2      1541.0        70       6490       2766.4  fread                 \n",
      "      0.0             6680          1      6680.0      6680.0      6680       6680          0.0  pipe2                 \n",
      "      0.0             6090         64        95.2        85.0        40        360         60.5  pthread_mutex_trylock \n",
      "      0.0             2330          1      2330.0      2330.0      2330       2330          0.0  bind                  \n",
      "      0.0             1290          1      1290.0      1290.0      1290       1290          0.0  listen                \n",
      "      0.0              380          1       380.0       380.0       380        380          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.5       2468317502          1  2468317502.0  2468317502.0  2468317502  2468317502          0.0  cudaDeviceSynchronize\n",
      "      4.8        125277243          3    41759081.0       75602.0       18290   125183351   72247542.8  cudaMallocManaged    \n",
      "      0.7         19214830          3     6404943.3     6102102.0     5862067     7250661     742181.2  cudaFree             \n",
      "      0.0            41700          1       41700.0       41700.0       41700       41700          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2468307670          1  2468307670.0  2468307670.0  2468307670  2468307670          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34159836   2304   14826.3    4382.5      1983     80320      22492.0  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11065352    768   14408.0    3743.0      1215     80832      22792.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report1.nsys-rep\n",
      "    /dli/task/report1.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-2294.qdstrm'\n",
      "[1/8] [========================100%] report3.nsys-rep\n",
      "[2/8] [========================100%] report3.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report3.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.2       1787307628         99  18053612.4  10071669.0      2420  100167994   26409876.0  poll                  \n",
      "      9.6        197377683         88   2242928.2   2065294.5       140   20609674    2848749.5  sem_timedwait         \n",
      "      2.1         43065570        497     86651.0     13530.0       410   10532586     601708.1  ioctl                 \n",
      "      1.0         20114646         24    838110.3      6290.0       820    7723669    2264055.1  mmap                  \n",
      "      0.0           943247         27     34935.1      4940.0      3310     545529     103292.4  mmap64                \n",
      "      0.0           535559         44     12171.8     11205.0      3590      31250       5399.6  open64                \n",
      "      0.0           183634          4     45908.5     42016.0     34251      65351      14875.8  pthread_create        \n",
      "      0.0           181465         29      6257.4      3720.0      1830      36451       7434.3  fopen                 \n",
      "      0.0           150591         11     13690.1     13960.0      1250      21250       5249.9  write                 \n",
      "      0.0            73100         12      6091.7      3295.0      1200      19530       6356.1  munmap                \n",
      "      0.0            50721         26      1950.8        70.0        60      48961       9588.2  fgets                 \n",
      "      0.0            38160          6      6360.0      7295.0      2410       8760       2644.7  open                  \n",
      "      0.0            37720         52       725.4       550.0       210       6340        860.2  fcntl                 \n",
      "      0.0            28731         22      1306.0      1020.0       550       3810        756.9  fclose                \n",
      "      0.0            24070         14      1719.3      1225.0       380       5230       1393.2  read                  \n",
      "      0.0            17551          2      8775.5      8775.5      4260      13291       6385.9  socket                \n",
      "      0.0            11880          1     11880.0     11880.0     11880      11880          0.0  connect               \n",
      "      0.0             9471          5      1894.2      1190.0        60       5911       2397.3  fread                 \n",
      "      0.0             7600          1      7600.0      7600.0      7600       7600          0.0  pipe2                 \n",
      "      0.0             6500         64       101.6       120.0        40        340         61.5  pthread_mutex_trylock \n",
      "      0.0             2730          1      2730.0      2730.0      2730       2730          0.0  bind                  \n",
      "      0.0             1470          1      1470.0      1470.0      1470       1470          0.0  listen                \n",
      "      0.0              380          1       380.0       380.0       380        380          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     52.6        126797649          3  42265883.0     61242.0     16250  126720157   73139550.2  cudaMallocManaged    \n",
      "     39.1         94180785          1  94180785.0  94180785.0  94180785   94180785          0.0  cudaDeviceSynchronize\n",
      "      8.4         20143517          3   6714505.7   6520109.0   5850798    7772610     975542.4  cudaFree             \n",
      "      0.0            49660          1     49660.0     49660.0     49660      49660          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         94171773          1  94171773.0  94171773.0  94171773  94171773          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.3         33687893   2306   14608.8    3103.0      1823     79392      22131.3  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.7         11061299    768   14402.7    3727.5      1375     80512      22786.3  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2306     0.175     0.020     0.004     1.036        0.296  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report3.nsys-rep\n",
      "    /dli/task/report3.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\n",
      "Number of SMs: 80\n",
      "Compute Capability Major: 8\n",
      "Compute Capability Minor: 6\n",
      "Warp Size: 32\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-1a37.qdstrm'\n",
      "[1/8] [========================100%] report4.nsys-rep\n",
      "[2/8] [========================100%] report4.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report4.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.9       1787715672         99  18057734.1  10075416.0     27980  100145389   26521155.0  poll                  \n",
      "      9.4        190770633         89   2143490.3   2068184.0       180   20378006    2356593.3  sem_timedwait         \n",
      "      1.7         34014918        497     68440.5     11250.0       390    8049623     434584.3  ioctl                 \n",
      "      1.0         19643784         24    818491.0      4440.0       890    7177988    2205882.6  mmap                  \n",
      "      0.0           887787         27     32881.0      3890.0      3150     544389     103029.7  mmap64                \n",
      "      0.0           506808         44     11518.4     10975.0      3710      30741       4868.8  open64                \n",
      "      0.0           186583         29      6433.9      3990.0      1490      35181       7336.4  fopen                 \n",
      "      0.0           185202          4     46300.5     43840.5     33850      63671      14443.8  pthread_create        \n",
      "      0.0           153914         11     13992.2     13891.0     11820      16671       1684.4  write                 \n",
      "      0.0            56241         12      4686.8      3460.0      1170      19770       4962.0  munmap                \n",
      "      0.0            50731         26      1951.2        70.0        50      48991       9594.3  fgets                 \n",
      "      0.0            42111          6      7018.5      7935.0      3160       9511       2506.8  open                  \n",
      "      0.0            35390         52       680.6       495.0       160       6690        883.3  fcntl                 \n",
      "      0.0            31970         22      1453.2      1075.0       510       4320        876.1  fclose                \n",
      "      0.0            21690         14      1549.3      1210.0       840       4420       1019.9  read                  \n",
      "      0.0            18560          2      9280.0      9280.0      4290      14270       7056.9  socket                \n",
      "      0.0            11070          1     11070.0     11070.0     11070      11070          0.0  connect               \n",
      "      0.0             8571          5      1714.2      1730.0        90       3851       1610.1  fread                 \n",
      "      0.0             6850          1      6850.0      6850.0      6850       6850          0.0  pipe2                 \n",
      "      0.0             5470         64        85.5        50.0        40        170         47.7  pthread_mutex_trylock \n",
      "      0.0             2690          1      2690.0      2690.0      2690       2690          0.0  bind                  \n",
      "      0.0             1800          1      1800.0      1800.0      1800       1800          0.0  listen                \n",
      "      0.0              280          1       280.0       280.0       280        280          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     49.1        111303614          3  37101204.7     27550.0     14381  111261683   64224858.5  cudaMallocManaged    \n",
      "     42.1         95381372          1  95381372.0  95381372.0  95381372   95381372          0.0  cudaDeviceSynchronize\n",
      "      8.7         19695314          3   6565104.7   6631009.0   5852886    7211419     681660.1  cudaFree             \n",
      "      0.0           106891          1    106891.0    106891.0    106891     106891          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         95434347          1  95434347.0  95434347.0  95434347  95434347          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.3         33696150   2310   14587.1    3119.0      1823     79615      22123.8  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.7         11063310    768   14405.4    3759.5      1343     80736      22790.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2310     0.174     0.020     0.004     1.036        0.296  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report4.nsys-rep\n",
      "    /dli/task/report4.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating '/tmp/nsys-report-a9f2.qdstrm'\n",
      "[1/8] [========================100%] report8.nsys-rep\n",
      "[2/8] [========================100%] report8.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report8.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     80.6        541726040         38  14255948.4  10068452.0     27571  100140983   21705239.3  poll                  \n",
      "     12.2         82281413         34   2420041.6   2064009.0       120   20479630    4309413.5  sem_timedwait         \n",
      "      5.7         38007919        483     78691.3     13460.0       400    9256664     522009.6  ioctl                 \n",
      "      1.1          7601187         18    422288.2      4670.0      1140    7471044    1759163.9  mmap                  \n",
      "      0.2          1133120         27     41967.4      4200.0      3380     739722     140327.5  mmap64                \n",
      "      0.1           525306         44     11938.8     10735.0      4690      31891       5149.4  open64                \n",
      "      0.0           192682          4     48170.5     47510.5     37430      60231      12245.9  pthread_create        \n",
      "      0.0           190955         29      6584.7      3820.0      1490      44471       8426.2  fopen                 \n",
      "      0.0           146241         11     13294.6     13770.0      2770      20580       4986.8  write                 \n",
      "      0.0            58380         26      2245.4        90.0        70      56230      11010.7  fgets                 \n",
      "      0.0            48210          7      6887.1      4720.0      3240      20150       5987.7  munmap                \n",
      "      0.0            40840          6      6806.7      6905.0      3480       9660       2362.4  open                  \n",
      "      0.0            35351         52       679.8       545.0       160       5550        731.7  fcntl                 \n",
      "      0.0            32282         22      1467.4      1235.0       750       3880        737.3  fclose                \n",
      "      0.0            20390         14      1456.4      1155.0       500       3880       1040.0  read                  \n",
      "      0.0            17191          2      8595.5      8595.5      3770      13421       6824.3  socket                \n",
      "      0.0            14090          1     14090.0     14090.0     14090      14090          0.0  connect               \n",
      "      0.0             9110          5      1822.0      1490.0        90       3700       1775.2  fread                 \n",
      "      0.0             5981          1      5981.0      5981.0      5981       5981          0.0  pipe2                 \n",
      "      0.0             5590         64        87.3        50.0        40        480         67.7  pthread_mutex_trylock \n",
      "      0.0             2220          1      2220.0      2220.0      2220       2220          0.0  bind                  \n",
      "      0.0             1540          1      1540.0      1540.0      1540       1540          0.0  listen                \n",
      "      0.0              150          1       150.0       150.0       150        150          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     83.9        116120559          1  116120559.0  116120559.0  116120559  116120559          0.0  cudaMallocManaged    \n",
      "     10.6         14605032          1   14605032.0   14605032.0   14605032   14605032          0.0  cudaDeviceSynchronize\n",
      "      5.5          7578756          1    7578756.0    7578756.0    7578756    7578756          0.0  cudaFree             \n",
      "      0.0            44471          1      44471.0      44471.0      44471      44471          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)            Name          \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------\n",
      "    100.0         14602092          1  14602092.0  14602092.0  14602092  14602092          0.0  deviceKernel(int *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11082131    768   14429.9    3759.5      1439     80768      22782.7  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report8.nsys-rep\n",
      "    /dli/task/report8.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-d526.qdstrm'\n",
      "[1/8] [========================100%] report9.nsys-rep\n",
      "[2/8] [========================100%] report9.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report9.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.4       1786831510         99  18048803.1  10071688.0      2640  100138764   26450462.4  poll                  \n",
      "      9.4        191500807         88   2176145.5   2066619.0       110   20422889    2536213.2  sem_timedwait         \n",
      "      1.9         39179211        497     78831.4     12610.0       380    8812336     497030.3  ioctl                 \n",
      "      1.2         23651322         24    985471.8      4370.0       930   10332771    2728468.9  mmap                  \n",
      "      0.1          1094176         27     40525.0      3820.0      2870     752582     142899.8  mmap64                \n",
      "      0.0           475821         44     10814.1     10405.5      4270      29420       4122.1  open64                \n",
      "      0.0           193833          4     48458.3     46325.5     37051      64131      12854.3  pthread_create        \n",
      "      0.0           192775         29      6647.4      4160.0      1410      32561       7338.2  fopen                 \n",
      "      0.0           143232         11     13021.1     13210.0      1050      22750       5892.0  write                 \n",
      "      0.0            59582         26      2291.6        90.0        70      57411      11242.2  fgets                 \n",
      "      0.0            56441         11      5131.0      4790.0      1090      16760       4162.6  munmap                \n",
      "      0.0            53060         22      2411.8      1505.0       750      19900       3985.0  fclose                \n",
      "      0.0            43961          6      7326.8      8135.0      3521      10020       2592.8  open                  \n",
      "      0.0            37171         52       714.8       470.0       150       6810        921.9  fcntl                 \n",
      "      0.0            20710         14      1479.3      1105.0       420       4040       1182.2  read                  \n",
      "      0.0            16850          2      8425.0      8425.0      4050      12800       6187.2  socket                \n",
      "      0.0            12100          1     12100.0     12100.0     12100      12100          0.0  connect               \n",
      "      0.0             9871          5      1974.2      1420.0        90       4961       2032.2  fread                 \n",
      "      0.0             7700          1      7700.0      7700.0      7700       7700          0.0  pipe2                 \n",
      "      0.0             6740         64       105.3       130.0        50        270         50.3  pthread_mutex_trylock \n",
      "      0.0             2210          1      2210.0      2210.0      2210       2210          0.0  bind                  \n",
      "      0.0             1241          1      1241.0      1241.0      1241       1241          0.0  listen                \n",
      "      0.0              270          1       270.0       270.0       270        270          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     49.7        117268209          3  39089403.0     24171.0     13340  117230698   67672346.8  cudaMallocManaged    \n",
      "     40.3         95055879          1  95055879.0  95055879.0  95055879   95055879          0.0  cudaDeviceSynchronize\n",
      "     10.1         23774695          3   7924898.3   7255521.0   6093601   10425573    2242218.7  cudaFree             \n",
      "      0.0            45431          1     45431.0     45431.0     45431      45431          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         95047152          1  95047152.0  95047152.0  95047152  95047152          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.3         33718457   2322   14521.3    2847.0      1822     79424      22092.3  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.7         11068084    768   14411.6    3727.5      1343     80607      22789.8  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2322     0.173     0.012     0.004     1.036        0.296  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report9.nsys-rep\n",
      "    /dli/task/report9.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-cece.qdstrm'\n",
      "[1/8] [========================100%] report10.nsys-rep\n",
      "[2/8] [========================100%] report10.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report10.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     80.4        491524714         33  14894688.3  10068707.0      2040  100129715   23316814.7  poll                  \n",
      "     10.9         66394903         29   2289479.4   2064834.0       210   20431919    4101101.0  sem_timedwait         \n",
      "      5.8         35429236        497     71286.2     10660.0       390    8054254     474155.6  ioctl                 \n",
      "      2.6         15882726         24    661780.3      5280.0       910    7235021    1843539.9  mmap                  \n",
      "      0.1           903494         27     33462.7      3830.0      2920     559689     106096.4  mmap64                \n",
      "      0.1           461570         44     10490.2      9780.0      3430      23820       3902.3  open64                \n",
      "      0.0           166323          4     41580.8     41585.5     33561      49591       6901.5  pthread_create        \n",
      "      0.0           149143         29      5142.9      3880.0      1710      20591       4554.2  fopen                 \n",
      "      0.0           138292         11     12572.0     13870.0      1090      16850       4599.5  write                 \n",
      "      0.0            60191         26      2315.0        70.0        60      58301      11419.0  fgets                 \n",
      "      0.0            49300         11      4481.8      3490.0      1190      19150       4986.3  munmap                \n",
      "      0.0            34720         52       667.7       460.0       160       5410        756.5  fcntl                 \n",
      "      0.0            32230          6      5371.7      5005.0      2400       8830       2301.2  open                  \n",
      "      0.0            26010         22      1182.3      1075.0       530       3330        639.5  fclose                \n",
      "      0.0            21601         14      1542.9      1290.0       880       3330        764.1  read                  \n",
      "      0.0            17031          5      3406.2      1640.0       100       9360       3733.8  fread                 \n",
      "      0.0            11270          2      5635.0      5635.0      3760       7510       2651.7  socket                \n",
      "      0.0             7871          1      7871.0      7871.0      7871       7871          0.0  connect               \n",
      "      0.0             6631          1      6631.0      6631.0      6631       6631          0.0  pipe2                 \n",
      "      0.0             5500         64        85.9        50.0        40        190         46.9  pthread_mutex_trylock \n",
      "      0.0             2040          1      2040.0      2040.0      2040       2040          0.0  bind                  \n",
      "      0.0             1270          1      1270.0      1270.0      1270       1270          0.0  listen                \n",
      "      0.0              310          1       310.0       310.0       310        310          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     71.6        120479863          3  40159954.3     52001.0     14260  120413602   69501700.2  cudaMallocManaged    \n",
      "     19.0         31936600          2  15968300.0  15968300.0    859804   31076796   21366640.0  cudaDeviceSynchronize\n",
      "      9.4         15906725          3   5302241.7   4368363.0   4276671    7261691    1697552.1  cudaFree             \n",
      "      0.0            60671          4     15167.8     12485.0      4291      31410      12593.2  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                      Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ---------------------------------------------\n",
      "     97.3         31079831          3  10359943.7  10519954.0   9974579  10585298     335331.0  initWith(float, float *, int)                \n",
      "      2.7           856991          1    856991.0    856991.0    856991    856991          0.0  addArraysInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11079729    768   14426.7    3791.5      1439     80736      22783.6  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report10.nsys-rep\n",
      "    /dli/task/report10.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-4331.qdstrm'\n",
      "[1/8] [========================100%] report11.nsys-rep\n",
      "[2/8] [========================100%] report11.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report11.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     73.6        361104340         29  12451873.8  10066695.0      2500  100130275   18396348.4  poll                  \n",
      "     12.3         60417894         26   2323765.2   2060264.0       120   20471206    4221826.1  sem_timedwait         \n",
      "     10.4         51054033        500    102108.1     13980.0       400   13713676     748892.5  ioctl                 \n",
      "      3.2         15574635         24    648943.1      4445.0      1000    7180498    1812329.2  mmap                  \n",
      "      0.2          1114249         27     41268.5      4391.0      3490     744972     141381.4  mmap64                \n",
      "      0.1           531899         44     12088.6     11595.0      5370      30771       4874.0  open64                \n",
      "      0.0           197153          4     49288.3     47966.0     37860      63361      11429.1  pthread_create        \n",
      "      0.0           188491         29      6499.7      4570.0      1860      28960       5975.2  fopen                 \n",
      "      0.0           147504         11     13409.5     15570.0      1150      17851       4675.3  write                 \n",
      "      0.0            59471         26      2287.3        90.0        70      57181      11196.2  fgets                 \n",
      "      0.0            51001          6      8500.2      8125.5      3870      15470       4196.0  open                  \n",
      "      0.0            41721         12      3476.8      2945.5      1420       7620       1757.1  munmap                \n",
      "      0.0            38100         52       732.7       540.0       160       6240        853.5  fcntl                 \n",
      "      0.0            35541         22      1615.5      1495.0       750       3400        767.6  fclose                \n",
      "      0.0            20711         14      1479.4      1185.0       360       4481       1278.9  read                  \n",
      "      0.0            18030          5      3606.0      1810.0        90      10040       3965.3  fread                 \n",
      "      0.0            15970          2      7985.0      7985.0      4210      11760       5338.7  socket                \n",
      "      0.0            11450          1     11450.0     11450.0     11450      11450          0.0  connect               \n",
      "      0.0             6930          1      6930.0      6930.0      6930       6930          0.0  pipe2                 \n",
      "      0.0             5550         64        86.7        50.0        40        180         46.3  pthread_mutex_trylock \n",
      "      0.0             2720          1      2720.0      2720.0      2720       2720          0.0  bind                  \n",
      "      0.0             1380          1      1380.0      1380.0      1380       1380          0.0  listen                \n",
      "      0.0              280          1       280.0       280.0       280        280          0.0  pthread_cond_broadcast\n",
      "      0.0              190          1       190.0       190.0       190        190          0.0  pthread_mutex_lock    \n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     87.2        128099253          3  42699751.0    29151.0     13870  128056232   73920881.3  cudaMallocManaged    \n",
      "     10.6         15568305          3   5189435.0  4225669.0   4137478    7205158    1746224.2  cudaFree             \n",
      "      1.1          1687817          1   1687817.0  1687817.0   1687817    1687817          0.0  cudaDeviceSynchronize\n",
      "      1.1          1580046          3    526682.0   531579.0    498928     549539      25658.4  cudaMemPrefetchAsync \n",
      "      0.0            37881          4      9470.3     5275.5      4280      23050       9080.2  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "     50.1           850879          1  850879.0  850879.0    850879    850879          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "     49.9           847678          3  282559.3  283679.0    280127    283872       2108.7  initWith(float, float *, int)                 \n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11074504    768   14419.9    3791.5      1439     80640      22780.4  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report11.nsys-rep\n",
      "    /dli/task/report11.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-28d8.qdstrm'\n",
      "[1/8] [========================100%] report12.nsys-rep\n",
      "[2/8] [========================100%] report12.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report12.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     74.3        360997711         29  12448196.9  10071696.0      2470  100128328   18660246.8  poll                  \n",
      "     12.1         58798296         26   2261472.9   2065228.5       300   20539378    4254529.3  sem_timedwait         \n",
      "      9.8         47851040        500     95702.1     13090.5       400    9951564     621877.0  ioctl                 \n",
      "      3.2         15651428         24    652142.8      6930.0      1100    7233319    1821732.4  mmap                  \n",
      "      0.2          1180200         27     43711.1      4290.0      2920     805524     152984.9  mmap64                \n",
      "      0.1           524428         44     11918.8     10850.5      4900      36191       5720.4  open64                \n",
      "      0.0           205142         29      7073.9      3650.0      1440      47760       9487.4  fopen                 \n",
      "      0.0           176753          4     44188.3     46500.5     26481      57271      14762.6  pthread_create        \n",
      "      0.0           143303         11     13027.5     12600.0      1220      20661       5249.0  write                 \n",
      "      0.0            77863         11      7078.5      4510.0      1940      34971       9413.4  munmap                \n",
      "      0.0            57841         26      2224.7        90.0        70      55601      10886.7  fgets                 \n",
      "      0.0            46432          6      7738.7      8705.5      3310      10120       2741.0  open                  \n",
      "      0.0            37910         52       729.0       540.0       150       6800        905.2  fcntl                 \n",
      "      0.0            33400         22      1518.2      1275.0       750       4800        873.9  fclose                \n",
      "      0.0            21220         14      1515.7      1260.0       450       4500       1197.9  read                  \n",
      "      0.0            12661          5      2532.2      1790.0        80       6011       2246.4  fread                 \n",
      "      0.0            11920          2      5960.0      5960.0      4090       7830       2644.6  socket                \n",
      "      0.0             9540          1      9540.0      9540.0      9540       9540          0.0  connect               \n",
      "      0.0             7060          1      7060.0      7060.0      7060       7060          0.0  pipe2                 \n",
      "      0.0             6080         64        95.0        50.0        40        430         66.3  pthread_mutex_trylock \n",
      "      0.0             2450          1      2450.0      2450.0      2450       2450          0.0  listen                \n",
      "      0.0             2160          1      2160.0      2160.0      2160       2160          0.0  bind                  \n",
      "      0.0              310          1       310.0       310.0       310        310          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     86.7        125742849          3  41914283.0    70801.0     31130  125640918   72509395.6  cudaMallocManaged    \n",
      "     10.8         15664497          3   5221499.0  4257210.0   4148718    7258569    1764988.2  cudaFree             \n",
      "      1.3          1917881          3    639293.7   526649.0    517588     873644     203003.9  cudaMemPrefetchAsync \n",
      "      1.2          1696098          1   1696098.0  1696098.0   1696098    1696098          0.0  cudaDeviceSynchronize\n",
      "      0.0            46221          4     11555.3     5695.0      4240      30591      12709.9  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "     50.5           863806          1  863806.0  863806.0    863806    863806          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "     49.5           845023          3  281674.3  282719.0    279552    282752       1838.1  initWith(float, float *, int)                 \n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11082574    768   14430.4    3775.5      1439     80768      22792.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report12.nsys-rep\n",
      "    /dli/task/report12.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](09-saxpy/01-saxpy.cu). It currently works and you can compile, run, and then profile it with `nsys profile` below.\n",
    "\n",
    "Record the runtime of the `saxpy` kernel without making any modifications and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200,000 ns*. Check out [the solution](09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
